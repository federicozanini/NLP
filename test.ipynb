{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMoMtTA543dn9jAhI+Tv941",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/federicozanini/NLP/blob/master/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF6OmPYovHzw"
      },
      "source": [
        "# A\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5moM9p3wTZD"
      },
      "source": [
        "git clone https://github.com/YOUR-USERNAME/YOUR-REPOSITORY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NJJ3hbso3LU"
      },
      "source": [
        "import os\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import json\r\n",
        "\r\n",
        "def load_data(path):\r\n",
        "\r\n",
        "    # DATAFRAME ROWS\r\n",
        "    dataframerows = []\r\n",
        "\r\n",
        "    with open(path) as f:\r\n",
        "        data = json.load(f)\r\n",
        "\r\n",
        "        for el in data['data']:\r\n",
        "            title = el['title']\r\n",
        "            paragraphs = el['paragraphs']\r\n",
        "\r\n",
        "            for context_qas in paragraphs:\r\n",
        "                  context = context_qas['context']\r\n",
        "                  qas = context_qas['qas']\r\n",
        "\r\n",
        "                  for a_q in qas: #[0][\"answers\"][0][\"text\"]\r\n",
        "                      answer = a_q['answers']\r\n",
        "                      question = a_q['question']\r\n",
        "                      #id = a_q['id']\r\n",
        "\r\n",
        "                      for a in answer:\r\n",
        "                          text = a['text']\r\n",
        "                          #answer_start = int(a['answer_start'])\r\n",
        "                          #answer_end = answer_start+len(text)-1\r\n",
        "\r\n",
        "                          # CREATE ROW\r\n",
        "                          row = {\r\n",
        "                              \"title\" : title,\r\n",
        "                              #\"paragraphs\" : paragraphs,\r\n",
        "                              #\"context_qas\" : context_qas,\r\n",
        "                              \"context\" : context,\r\n",
        "                              #\"qas\" : qas,\r\n",
        "                              \"id\" : id,\r\n",
        "                              \"question\" : question,\r\n",
        "                              #\"answer\" : answer,\r\n",
        "                              \"answer_text\" : text,\r\n",
        "                              #\"answer_start\" : answer_start,\r\n",
        "                              #\"answer_end\":answer_end\r\n",
        "                              }\r\n",
        "\r\n",
        "                          # APPEND ROW TO DATAFRAME ROWS\r\n",
        "                          dataframerows.append(row)\r\n",
        "\r\n",
        "        df = pd.DataFrame(dataframerows)\r\n",
        "        df = df[[\"title\", \"context\", \"question\", \"answer_text\"]] ##, \"answer_start\", \"answer_end\"]]\r\n",
        "\r\n",
        "        return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfAcJyXvpGtz"
      },
      "source": [
        "path = 'SQuAD/Dataset/training_set.json'\r\n",
        "df = load_data(path)\r\n",
        "df.head(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV4MoK2f83P6"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkdP6Fbi89Cs"
      },
      "source": [
        "## using bert tokenizer to encode data \r\n",
        "import tensorflow as tf\r\n",
        "from transformers import BertTokenizer\r\n",
        "\r\n",
        "\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFdxknAy9AUg"
      },
      "source": [
        "#cella per analizzare tokenizer.encode\r\n",
        "input_ids = tokenizer.encode(\"my name is matteo 105, luca!1 cazzo\")\r\n",
        "\r\n",
        "print(input_ids)\r\n",
        "\r\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\r\n",
        "\r\n",
        "# For each token and its id...\r\n",
        "for token, id in zip(tokens, input_ids):\r\n",
        "    \r\n",
        "    # If this is the [SEP] token, add some space around it to make it stand out.\r\n",
        "    if id == tokenizer.sep_token_id:\r\n",
        "        print('')\r\n",
        "    \r\n",
        "    # Print the token string and its ID in two columns.\r\n",
        "    print('{:<12} {:>6,}'.format(token, id))\r\n",
        "\r\n",
        "    if id == tokenizer.sep_token_id:\r\n",
        "        print('')\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFMMzyQg9CZs"
      },
      "source": [
        "# little utils func, find the index of a sequence inside a list \r\n",
        "def find_sublist(l1, l2):\r\n",
        "  for i, el in enumerate(l1):\r\n",
        "    if l1[i:i+len(l2)]==l2:\r\n",
        "      return i,i+len(l2)-1   \r\n",
        "  return -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiJBaAu39EcB"
      },
      "source": [
        "'''\r\n",
        "\r\n",
        "DATA GENERATOR, objective is to suply the net with batch belonging to the same Context\r\n",
        "\r\n",
        "'''\r\n",
        "import random \r\n",
        "\r\n",
        "def data_loader(df):\r\n",
        "  title = []\r\n",
        "  while True:   \r\n",
        "    if title == []:\r\n",
        "      title = list(df['title'].unique())    \r\n",
        "    \r\n",
        "    current = title.pop(random.randrange(len(title)))\r\n",
        "    df0 = df[df['title']==current]\r\n",
        "    \r\n",
        "    encoded = []\r\n",
        "    max_len = 0\r\n",
        "    for i, t in df0.iterrows():\r\n",
        "      encoded.append(tokenizer.encode(t[2],t[1]))\r\n",
        "    \r\n",
        "    #padding input sequence \r\n",
        "    padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(encoded, padding=\"post\")\r\n",
        "\r\n",
        "    X = np.array(padded_inputs)\r\n",
        "\r\n",
        "    Y_start = np.zeros(X.shape, dtype='int32')\r\n",
        "    Y_end  = np.zeros(X.shape, dtype='int32')\r\n",
        "\r\n",
        "    for i, t in enumerate(df0['answer_text']):\r\n",
        "      enc_ans = tokenizer.encode(t,add_special_tokens= False) \r\n",
        "      try:\r\n",
        "        start, end = find_sublist(X[i].tolist(),enc_ans)\r\n",
        "      except:\r\n",
        "        print(len(X[i].tolist()))\r\n",
        "        print(type(X[i].tolist()))\r\n",
        "        print(X[i].tolist())\r\n",
        "      Y_start[i,start]=1\r\n",
        "      Y_start[i,end]=1\r\n",
        "\r\n",
        "    yield(X,Y_start,Y_end)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sqn2N2r9GZx"
      },
      "source": [
        "X,Y_start,Y_end = next(data_loader(df))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEqqy0On9IEJ"
      },
      "source": [
        "help(tokenizer.encode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f9YCk019PpB"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f3N6KFa9RBL"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\r\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\r\n",
        "        super(TransformerBlock, self).__init__()\r\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\r\n",
        "        self.ffn = keras.Sequential(\r\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\r\n",
        "        )\r\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\r\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\r\n",
        "        self.dropout1 = layers.Dropout(rate)\r\n",
        "        self.dropout2 = layers.Dropout(rate)\r\n",
        "\r\n",
        "    def call(self, inputs, training):\r\n",
        "        attn_output = self.att(inputs, inputs)\r\n",
        "        attn_output = self.dropout1(attn_output, training=training)\r\n",
        "        out1 = self.layernorm1(inputs + attn_output)\r\n",
        "        ffn_output = self.ffn(out1)\r\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\r\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}